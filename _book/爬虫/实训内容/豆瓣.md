# 豆瓣电影top250爬取
```
# 电影的名字和链接
from lxml import etree
from urllib.request import *
import pickle     # 持久化
import time
arr = []

# url = "https://movie.douban.com/top250?start="
# urls = [url+str(i) for i in range(0,250,25)]
#
# def spider(link):
#     time.sleep(1)
#     print("正在爬取:%s"%link)     # 每一次执行的时候给一个输出：正在爬取哪个页面（0,25,50,75...）第一个link是0，第二个link是25...
#     with urlopen(link) as html:
#         text = html.read().decode("utf-8")        # 文本格式的html
#     doc = etree.HTML(text)        # 转换成dom对象
#
#     titles = doc.xpath("//ol[@class='grid_view']/li/div/div[@class='info']/div[@class='hd']/a/span[1]/text()")
#     links = doc.xpath("//ol[@class='grid_view']/li/div/div[@class='info']/div[@class='hd']/a/@href")
#
#     arr.append(list(zip(titles,links)))       # zip就是一一对应，然后转换成列表，然后存储下来，需要一个全局的arr放入内容，调用arr.append推送
#
# for link in urls:
#     spider(link)      # 循环执行link
#
# with open("./top250.txt",'wb') as f:        # 在当前的文件中打开一个top.250文件，以二进制写入的方式
#     pickle.dump(arr,f)        # 把arr存储到f中，内容就被写过来了


# with open("./top250.txt",'rb') as f:        # 以二进制读的形式打开
#     obj = pickle.load(f)      # 把f文件中的内容加载出来
#
# print(len(obj))     # 一共10页
#
# for item in obj:
#     print(item)     # 一个大列表，里边10个小列表
```
```
### 爬取豆瓣电影详情
from urllib.request import *
import pickle,fake_useragent,xlwt
from lxml.etree import *
import time

with open('top250.txt','rb') as f:
    arr = pickle.load(f)

lists = []
for arr1 in arr:
    for title,url in arr1:
        lists.append(url)

ua = fake_useragent.UserAgent()
header = {
    'User-Agent':ua.random
}

def spider(url):
    time.sleep(1)
    req = Request(url,headers=header)
    with urlopen(req) as html:
        text = html.read().decode()
    doc = HTML(text)
    # 导演
    pl1 = "/".join(doc.xpath("//div[@id='info']/span[1]/span[@class='attrs']/a[1]/text()"))
    # 编剧
    pl2 = "/".join(doc.xpath("//div[@id='info']/span[2]/span[@class='attrs']/a/text()"))
    # 主演
    pl3 = "/".join(doc.xpath("//div[@id='info']/span[3]/span[@class='attrs']/a/text()"))
    # 类型
    pl4 = "/".join(doc.xpath("//div[@id='info']/span[@property='v:genre']/text()"))
    # 剧情简介
    pl5 = doc.xpath("//span[@property='v:summary']/text()")[0].strip()
    print("正在爬取:%s" % pl1)
    return {
        # 'dir':pl1,
        # 'edit':pl2,
        # 'actor':pl3,
        # 'type':pl4,
        # 'dis':pl5
         pl1,
         pl2,
         pl3,
         pl4,
         pl5
    }

for url in lists:
    res = list(spider(url))
    print(res)
```